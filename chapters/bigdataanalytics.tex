BigData, according to Wikipedia (Sep 2015), is “the term for a collection of data set so large and complex that it becomes difficult to process using on hand database management tools or traditional data processing applications”. The data-sets not only contain structured databases, but also include unstructured databases such as social media data or GPS (Global Positioning System) data.
\par BigData comes from everywhere influence our life, and so is too big,complex and moves too fast. For example, posting pictures and writing comments on Facebook; uploading and watching videos on YouTube; sending and receiving messages through smart phones;sending voice messages through Whatspapp all count as BigData. To analyse BigData, new analytical methods have to be developed to feed business, government and organization needs.
\par Distributed computing and  parallel processing techniques are widely used in industry for BigData applications. Hadoop (High-availability distributed object oriented platform), the most  popular open-source platform for reliable, scalable, distributed computing, is often referred to by BigData researchers. Two main core frameworks in Hadoop: Hadoop Distributed File System (HDFS) and MapReduce, have being deployed in industries for the management of cluster distributed data centers such as Facebook, Google, Yahoo, Amazon. com, eBay and Twitter (hadoop.appache.org).
\section{3V’s Model And New V’s for Bigdata}
According to Gartner 3Vs definition[3], Volume, velocity and variety to characterize the concept of Big Data.
\subsection{Volume}
90\% of all data ever created was created in the past 2 years. From now on, the amount of data in the world will double every two years. By 2020, we will have 50 times the amount of data as that we had in 2011. The sheer volume of the data is enormous and a very large contributor to the ever expanding digital universe is the Internet of Things with sensors all over the world in all devices creating data every second. The era of a trillion sensors is upon us.
If we look at airplanes they generate approximately 2,5 billion Terabyte of data each year from the sensors installed in the engines.  Self-driving cars will generate 2 Petabyte of data every year. Also the agricultural industry generates massive amounts of data with sensors installed in tractors. Shell uses super-sensitive sensors to find additional oil in wells and if they install these sensors at all 10.000 wells they will collect approximately 10 ExaByte of data annually. That again is absolutely nothing if we compare it to the Square Kilometer Array Telescope that will generate 1 Exabyte of data per day.
\par In the past, the creation of so much data would have caused serious problems. Nowadays, Bigdata technologies allows us  to manipulate such a huge volume of data
\subsection{Velocity}
The Velocity is the speed at which the data is created, stored, analysed and visualized. In the past, when batch processing was common practice, it was normal to receive an update from the database every night or even every week. Computers and servers required substantial time to process the data and update the databases. In the big data era, data is created in realtime or near real-time. With the availability of Internet connected devices, wireless or wired, machines and devices can pass-on their data the moment it is created.
\par
The speed at which data is created currently is almost unimaginable: Every minute we upload 100 hours of video on YouTube. In addition, every minute over 200 million emails are sent, around 20 million photos are viewed and 30.000 uploaded on Flickr, almost 300.000 tweets are sent and almost 2,5 million queries on Google are performed.
\subsection{Variety}
In the past, all data that was created was structured data, it neatly fitted in columns and rows but those days are over. Nowadays, 90\% of the data that is generated by organisation is unstructured data. Data today comes in many different formats: structured data, semi-structured data, unstructured data and even complex structured data. The wide variety of data requires a different approach as well as different techniques to store all raw data.
\par
There are many different types of data and each of those types of data require different types of analysis or different tools to use. Social media like Facebook posts or Tweets can give different insights, such as sentiment analysis on your brand, while sensory data will give you information about how a product is used and what the mistakes are.

\section{The Four Additional V’s}
Now that the context is set regarding the traditional V’s, let’s see which other V’s are important for organisations to keep in mind when they develop a big data strategy.
\subsection{Veracity}
Having a lot of data in different volumes coming in at high speed is worthless if that data is incorrect. Incorrect data can cause a lot of problems for organisations as well as for consumers. Therefore, organisations need to ensure that the data is correct as well as the analysis performed on the data are correct. Especially in automated decision-making, where no human is involved anymore, you need to be sure that both the data and the analyses are correct.
\par
IBM coined Veracity as the fourth V, which represents the unreliability inherent in some sources of data. For example, customer sentiments in social media are uncertain in nature, since they entail human judgment. Yet they contain valuable information. Thus the need to deal with imprecise and uncertain data is another facet of big data, which is addressed using tools and analytics developed for management and mining of uncertain data.
\subsection{Variability}
Big data is extremely variable. Brian Hopkins, a Forrester principal analyst, defines variability as the “variance in meaning, in lexicon”. He refers to the supercomputer Watson who won Jeopardy. The supercomputer had to “dissect an answer into its meaning and to figure out what the right question was”. That is extremely difficult because words have different meanings an all depends on the context. For the right answer, Watson had to understand the context.
\par
Variability means that the meaning is changing (rapidly). In (almost) the same tweets a word can have a totally different meaning. In order to perform a proper sentiment analyses, algorithms need to be able to understand the context and be able to decipher the exact meaning of a word in that context. This is still very difficult. SAS introduced Variability as additional dimensions of big data. Variability refers to the variation in the data flow rates. Often, big data velocity is not consistent and has periodic peaks and troughs.This imposes a critical challenge: the need to connect, match, cleanse and transform data received from different sources.
\subsection{Value}
All that available data will create a lot of value for organisations, societies and consumers. Big data means big business and every industry will reap the benefits from big data. It is estimated that potential annual value of big data to the US Health Care is \$300 billion, more than double the total annual health care spending of Spain. They also mention that big data has a potential annual value of € 250 billion to the Europe’s public sector administration. Even more, in their well-regarded report from 2011, they state that the potential annual consumer surplus from using personal location data globally can be up to \$ 600 billion in 2020. That is a lot of value.
\par
Of course, data in itself is not valuable at all. The value is in the analyses done on that data and how the data is turned into information and eventually turning it into knowledge. The value is in how organisations will use that data and turn their organisation into an information-centric company that relies  on insights derived from data analyses for their decision-making.
\subsection{Visualization}
This is the hard part of big data. Making all that vast amount of data comprehensible in a manner that is easy to understand and read. With the right analyses and visualizations, raw data can be put to use otherwise raw data remains essentially useless. Visualization of course do not mean ordinary graphs or pie charts. They mean complex graphs that can include many variables of data while still remaining understandable and readable.
\par
Visualizing might not be the most technological difficult part; it sure is the most challenging part. Telling a complex story in a graph is very difficult but also extremely crucial. Luckily there are more and more big data start-ups appearing that focus on this aspect and in the end, visualizations will make the difference.

