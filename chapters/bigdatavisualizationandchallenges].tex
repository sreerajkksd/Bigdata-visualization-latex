Visualization is an essential tool for making sense of big data.It provides a far richer view of big data than can be obtained  from tables and statistics alone. However, the key to effective analysis of big data is the integration of visualization into analytics tools so that all kinds of users can interpret big data from a wide range of sources — click streams, social media, log files, videos and more.
\par
Online marketplace eBay, have hundreds of million active users and billions of goods sold each month, and they generate a lot of data. To make all that data understandable, eBay turned to Big Data visualization tool: Tableau, which has capability to transform large, complex data sets into intuitive pictures. The results are also inter-active. Based on them, eBay employees can visualize search relevance and quality to monitor the latest customer feedback and conduct sentiment analysis.
\section{Challenges in Big Data Visualization}
For Big Data applications, it is particularly difficult to conduct data visualization because of the large size and high dimension of Big Data. However, current Big Data visualization tools mostly have poor performances in functionalities, scalability and response time. What we need to do is rethinking the way we visualize Big Data, not like the way we adopt before. For example, the history mechanisms for information visualization also are data-intensive and need more efficient approaches. Uncertainty can lead to a great challenge to effective uncertainty-aware visualization and arise in any stage of a visual analytics process. New framework for modeling uncertainty and characterizing the evolution of the uncertainty information are highly necessary through analytical processes.
\par Major challenges in visual analytics are
\subsection{Scalability}
Scalability represents the scalability of visual representations.A visual analytics tool which can be used for lower dimensions of data may not be appropriate for visualizing higher dimensions of data.Since Bigdata applications are involving high dimensions of data we need tools that can operate on these data quickly and effectively.
\par 
Perceptual and interactive scalability are also challenges of big data visualization. Visualizing every data point can lead to over-plotting and may overwhelm users’ perceptual and cognitive capacities; reducing the data through sampling or filtering can elide interesting structures or outliers. Querying large data stores can result in high latency, disrupting fluent interaction
\par 
Scalability of the visualization tool should answer the following questions if it is considered scalable.
\begin{itemize}
	\item How to run queries on distributed systems to explore big data sets?
	\item How to visualize a million multi-variate items on a screen?
	\item How to lower the time needed to run a clustering algorithm on xGbytes?
	\item How to design an interactive user interface loading big data in < 1 sec?
\end{itemize}
\subsection{Dynamics}
Visualization tool should be answer the following questions if it is considered to be dynamic.
\begin{itemize}
	\item How to aggregate data streams?
	\item How to visualize a continuously changing data structure?
	\item How to adapt clustering algorithms to consider dynamic data?
	\item How to design an interactive user interface continuously fed by data?
\end{itemize}
\subsection{Need Of Massive Parallelization}
Due to increasing data sizes and the emergence of the Big Data problem, the need for massive parallelization is a driving visualization research challenge. Supercomputing simulations regularly generate massive data sets with billions of data points per time step. Parallelization is an effective way of dealing with such data: there is more memory for storing data, there is more compute power for executing algorithms, and there is often more I/O bandwidth for reading data. The basic challenge for parallel visualization algorithms is to decompose the problem into independent tasks that can be run concurrently on all of the processing elements (i.e., the instances of the program), thus avoiding idle time.
\par 
Data parallelism is the dominant technique; data sets are decomposed into pieces and the pieces are partitioned over the processing elements. This approach has been shown to be highly scalable with results for hundreds of thousands of processing elements in research prototypes and tens of thousands of processing elements in production software .The role of visualization software, with respect to parallelization, is to provide a framework that shields algorithm developers from complexity.
\subsection{Application Architecture and Data Management}
Application architecture refers to the system design of visualization software. Data management for visualization must provide visualization techniques that integrate into the data life cycle. Although these two topics are distinct, they are treated together here, since emerging data management needs will drive application architecture. Traditionally, data management has not been a pressing concern for visualization software. Data, whether observed or simulated, was stored in the file system for processing;visualization software simply read whatever data it needed from files whenever it needed it. However, increases in data size, observed and simulated, as well as diversity of data sources, mandate new approaches in data management. 
\par
Application architectures exist to solve the simplest use model: “have data, want a picture," where the application architecture serves as a black box that consumes data and produces imagery with user-selected methods and parameters.Twenty-five years ago, the architecture for most visualization applications was a single binary that read from the local file system and produced images using local graphics. A little over a decade ago, scientific visualization applications for large data shifted to a client-server design where data was processed by a remote parallel server, producing geometry that was rendered by a local client. Today, application architectures for visualization frequently involve web clients and remote data access. In short, application architectures evolve to meet evolving data management needs.
\section{Potential Solutions}
The potential solutions of the common problems in big data visualization is presented in following section.
\subsection{Meeting the Need for Speed}
Meeting the need for speed In today’s hyper-competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. Visualization helps organizations perform analyses and make decisions much more rapidly, but the challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem.Both approaches allow organizations to explore huge data volumes and gain business insights in near-real time.
\subsection{Dealing with Outliers}
The graphical representations of data made possible by visualization can communicate trends and outliers much faster than tables containing numbers and text.Outliers typically represent about 1 to 5 percent of data, but when you’re working with massive amounts of data, viewing 1 to 5 percent of the data is rather difficult. How do you represent those points without getting into plotting issues? Possible solutions are to remove the outliers from the data (and therefore from the chart) or to create a separate chart for the outliers. You can also bin the results to both view the distribution of data and see the outliers. While outliers may not be representative of the data, they may also reveal previously unseen and potentially valuable insights.
\subsection{Displaying meaningful results}
Plotting points on a graph for analysis becomes difficult when dealing with extremely large amounts of information or a variety of categories of information.For example, if we have 10 billion rows of retail SKU data that we’re trying to compare. The user trying to view 10 billion plots on the screen will have a hard time seeing so many data points. One way to resolve this is to cluster data into a higher-level view where smaller groups of data become visible. By grouping the data together, or “binning,” data can be more effectively visualized.
